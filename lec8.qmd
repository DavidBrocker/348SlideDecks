---
title: "Sampling Theory"
subtitle: "Lecture 8"
author: "Dave Brocker"
institute: "Farmingdale State College"
format: 
  revealjs:
    theme: custom.scss
    width: 1600
    height: 900
    incremental: true   
    touch: true
    scrollable: true
    chalkboard: true
filters:
  - webr
---

## Standard Normal Distribution

The 4 properties of a standard normal distribution are:

::::: columns
::: {.column width="50%"}
```{r}

library(ggplot2)
library(dplyr)

ggnorm <- function(n, m, sd) {
  x <- rnorm(n, m, sd)
  tibble(
    x = x,
    pd = dnorm(x, mean(x), sd(x))
  ) |>
    ggplot(aes(x, pd)) +
    geom_line() +
    geom_vline(aes(xintercept = mean(x)), 
               lty = "dashed") +
    theme_minimal() +
    scale_x_continuous(
      breaks = scales::pretty_breaks(n = 8)
      ) +
    labs(
      x = "\nValues",
      y = "Probability Density\n"
    )
}

ggnorm(1000,40,3)
```
:::

::: {.column width="50%"}
-   They are shaped like a bell ("bell curve").

-   They are symmetric.

-   They are unimodal.

-   The mean = median = mode.
:::
:::::

## Statistics

-   Descriptive Statistics:

    -   Goal: Describe the sample

        -   Examples: Mean, Standard Deviation

-   Inferential Statistics:

    -   Goal: Use the sample to make inferences about the population

        -   t-test
        -   ANOVA
        -   Regression

## Sampling theory

> Professor Brocker still wants to know how much adults in the US enjoy the Netflix Original, *Dark*. He has unlimited funds to study this **very** important research question. He hires 30 students to collect the data. Each student has to collect 500 responses to the following question:

## Sampling theory

> On a scale of 1 (I hate it with my entire being) to 10 (I believe in my soul that Dark is the best show ever made), how much do you enjoy Dark?

![](images/dark.png){style="border-radius: 15px;" fig-align="center" width="419"}

## Sampling Theory

-   Each one of you asks 500 people how much they enjoy the Netflix Original Dark.

-   This is you on the street asking people:

::: fragment
![](https://media.tenor.com/xoZUAVXAQuMAAAAM/frantic-running.gif){style="border-radius: 15px;" width="452"}
:::

## Sampling Theory

How many people did each of you ask?

$30 * 500 = 15,000$

```{r}
#| code-fold: true
library(dplyr)
library(tibble)
library(gt)
library(latex2exp)
# 30 Students in the class
n = 30

# Each student asks 500 people
full_sample = n*500

# Simulate Results
replicate(n = 30,
            rnorm(500,5,7)) |> round(0) |> 
  data.frame() |> 
  rename_with(.fn = function(x) paste0("S", 1:30)) |> 
  summarize_all(mean) |>
  tidyr::pivot_longer(cols = everything(),
                      names_to = "Sample #",
                      values_to = "Sample Mean") |> 
  gt_preview()

```

## Sampling theory

-   Once each of you collect *500 responses*, I ask you to calculate the average answer. So you calculate the mean (you add up all 500 responses, and then divide that number by 500).

::: fragment
-   $\frac{\sum(x_1 + x_2 + x_3...+ x_{500)}}{500}$

-   They are the means from each of your samples.

-   These are called **Sample Means**
:::

## Sampling Theory

### Here is our Ratings Data

```{r}
#| code-fold: true
library(ggplot2)
library(forcats)
library(gganimate)

tibble(
  ratings = rnorm(15000,6.6,2),
    dx = dnorm(ratings,mean(ratings),sd(ratings))
  ) |> 
  ggplot(aes(ratings, dx)) +
  geom_line() + 
  theme_minimal(base_size = 20) +
  labs(
    y = "Probability Density\n",
    x = "\nAverage Ratings for Dark"
  ) +
  geom_vline(aes(xintercept = mean(ratings)), lty = "dashed") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 8))


```

## Sampling theory

-   We can do lots of cool things with Sample Means.

    -   We could calculate the Mean of the Sample Means.

    -   We could calculate the standard deviation of the Sample Means.

-   But...it's theoretical. It doesn't really exist, but we imagine it's existence for the sake of **Sampling Theory**.

## Sampling distribution

::::: columns
::: {.column width="50%"}
-   A **Sampling Distribution** is the theoretical distribution of means across every single possible sample.

-   If we took samples of the population until we got every single person in the population, then calculated the mean for each sample, they would distribute like this:
:::

::: {.column width="50%"}
```{r}
tibble(
  ratings = rnorm(15000,6.6,2),
    dx = dnorm(ratings,mean(ratings),sd(ratings))
  ) |> 
  ggplot(aes(ratings, dx)) +
  geom_line() + 
  theme_minimal(base_size = 20) +
  labs(
    y = "Probability Density\n",
    x = "\nAverage Ratings for Dark"
  ) +
  geom_vline(aes(xintercept = mean(ratings)), lty = "dashed") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 8))
```
:::
:::::

## Sampling Theory

::::: columns
::: {.column width="50%"}
-   Except now each value in this distribution no longer represents 1 person/participant.

-   Each value in this distribution represents the average of 1 sample, a Sample Mean.

    -   In our case, each value is the average of 30 samples of 500 people.
:::

::: {.column width="50%"}
```{r}

new_dat <- tibble(
  z = seq(-4, 4, length.out = 1000),
  dx = dnorm(z)
)

labels <- tibble(
  x = c(-1, 1, -1.5, 1.5, -2.6, 2.6, -3.5, 3.5),
  y = c(.36,0.36, 0.25, 0.25, 0.075, 0.075, 0.025, 0.025),
  label = c("34.1%", "34.1%", "13.6%", "13.6%", "2.1%", "2.1%", "0.1%", "0.1%")
)


p <- 
  ggplot(new_dat, aes(x = z, y = dx)) +
  geom_line(size = 1.2) +
  geom_area(data = filter(new_dat, z >= -1 & z <= 1), fill = "lightblue", alpha = 0.6) +
  geom_area(data = filter(new_dat, z >=-1 & z <=0), fill = "lightblue", alpha = .6) +
  geom_area(data = filter(new_dat, z >=-2 & z <=-1), fill = "blue", alpha = .6) +
  geom_area(data = filter(new_dat, z <=2 & z >=1), fill = "blue", alpha = .6) +
  geom_area(data = filter(new_dat, z >=-4 & z <= -2), fill = "darkblue", alpha = .6) +
  geom_area(data = filter(new_dat, z >=2 & z <= 4), fill = "darkblue", alpha = .6) +
  geom_text(data = labels, aes(x = x, y = y, label = label), size = 4, inherit.aes = FALSE) +
  theme_minimal(base_size = 13) +
  labs(
    x = "\nStandard Deviations",
    y = "Probability Density\n"
  ) +
  scale_x_continuous(breaks = -4:4) +
  theme_sub_panel(grid = element_blank())

p
```
:::
:::::

## New Terminology

### Sample Mean and Sample Standard Deviation

::::: columns
::: {.column width="50%"}
-   We previously referred to the average as $\bar{x}$ and the standard deviation as $s$

-   The new terms refer to their specific attributes as a statistic of the sample

-   $\mu$ (pronounced like 'mew') represents the population mean

-   $\sigma$ represents the population standard deviation
:::

::: {.column width="50%"}
```{r}

library(tibble)

tibble(
  Term = c(
    "Sample Mean",
    "Sample SD",
    "Population Mean",
    "Population SD"
  ),
  Symbol = c(
    "$$\\bar{x}$$",
    "$$s$$",
    "$$\\mu$$",
    "$$\\sigma$$"
  ),
  Type = c(
    "Sample",
    "Sample",
    "Population",
    "Population"
  ),
  Formula = c(
    "$$\\frac{\\sum x_i}{n}$$",
    "$$\\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n-1}}$$",
    "$$\\frac{\\sum X_i}{N}$$",
    "$$\\sqrt{\\frac{\\sum (X_i - \\mu)^2}{N}}$$"
  ),
  Relation_to_Sample = c(
    "Mean of observed sample",
    "SD of sample values",
    "True (unknown) mean",
    "True (unknown) SD"
  )
) |> 
  gt() |> 
  fmt_markdown(columns = c(Formula,Symbol)) |> 
  opt_row_striping()

```
:::
:::::

## Sampling theory

-   Each value in this distribution no longer represents 1 person/participant.

-   Each value in this distribution now represents the average of 1 sample, a Sample Mean.

```{r}

tibble(
  ratings = rnorm(15000,6.6,2),
    dx = dnorm(ratings,mean(ratings),sd(ratings))
  ) |> 
  ggplot(aes(ratings, dx)) +
  geom_line() + 
  theme_minimal(base_size = 20) +
  labs(
    y = "Probability Density\n",
    x = "\nAverage Ratings for Dark"
  ) +
  geom_vline(aes(xintercept = mean(ratings)), lty = "dashed") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +
  annotate(geom = "point", x = 3, y = .04, color = "maroon") +
  geom_segment(x = 3, y = 0, xend = 3, yend = .036, size = .8) +
  theme_sub_panel(grid = element_blank())
```

## Review

-   What is a sample distribution?

-   What does each X value in a sample distribution represent?

-   What does $\mu$ represent?

-   What does $\sigma$ represent?

## Review

-   What is a sample distribution?

    -   A sample distribution represents how all possible samples from the population would be visualized

-   What does each X value in a sample distribution represent?

    -   That sample's average

-   What does $\mu$ represent?

    -   The population average

-   What does $\sigma$ represent?

    -   The population standard deviation

## Probability

-   What percentage of participants rated Dark with a z-score of 2 or higher?

```{r}
p
```

## Probability

-   What percentage of participants rated Dark with a z-score of 2 higher?

```{r}
p + 
  geom_segment(x = 2, xend = 2, y = 0, yend = .05) +
  geom_area(data = p$data |> filter(z <= 2), fill = "white")
```

## Probability

-   This is the distribution of **sample means** from adults in the US.

-   What is the probability of any sample having a mean that is a z-score of 2 or higher?

```{r}
p + 
  geom_segment(x = 2, xend = 2, y = 0, yend = .05) +
  geom_area(data = p$data |> filter(z <= 2), fill = "white")
```

## Assumptions of the Normal Curve

-   We *assume* that the distribution of sample means is normal.

-   We use that assumption to gauge the probability of getting a particular mean from a single sample.

-   We can literally find that probability, the same way we did with x-values in a normal distribution.

## Example

### Let Me Get Uhh....

We want to know how much people like pizza.

-   There are `12,500` people in our population. Each of the `25` of us collects a sample of `500`.

-   $500 \times 25 = 12,500$

-   Each of us calculates the mean response from our sample of 500 people.

-   What do we call those means?

    -   What would it look like if we were to plot it?

## Pizza Plot

### Visual Example

```{r}
set.seed(123)

df <- data.frame(
  sample_mean = replicate(
    10000,
    mean(rnorm(500, 4.2, 1))
  )
)

ggplot(df, aes(sample_mean)) +
  geom_histogram(
    aes(y = after_stat(density)),   # density instead of counts
    fill = "coral",
    bins = 40,
    alpha = 0.6
  ) +
  stat_function(
    fun = dnorm,
    args = list(
      mean = 4.2,
      sd = 1/sqrt(500)      # theoretical SE
    ),
    color = "blue",
    size = 1
  ) +
  theme_minimal() +
  labs(
    x = "Sample Mean",
    y = "Density",
    title = "Sampling Distribution of the Mean (n = 500)"
  )
  
```

## Example

I randomly choose a mean from our distribution of sample means, about how much people like pizza.

What is the probability of picking a mean with a z-score of -1 or less?

```{r}
p
```

## Example

I randomly choose a mean from our distribution of sample means, about how much people like pizza.

What is the probability of picking a mean with a z-score of -1 or less?

```{r}
p + 
  geom_area(data = p$data |> filter(z>=-1), fill = "white")
```

## Sampling theory

> Sampling Theory is the body of principles underlying the drawing of infinite samples that accurately represent the population from which they are taken and to which inferences can be made.

## Sampling Theory

### The basis for inferential statistics

-   Independent variable:

    -   Experimental Group: Super Secret Limitless Drug

-   Control Group: Placebo

    -   Dependent variable: IQ

## Sampling theory

> Sampling Theory is the idea that if we took infinite samples of a population, they would create a normal distribution.

-   If that is correct, we can then make guesses about the probability of getting a specific mean.

-   We will use this probability to gauge **significance** of our inferential statistics.

## Sampling Theory

### New Terminology (Again???)

-   The mean of a sample is notated as `M`.

-   The mean of a population is notated as ($\mu$).

-   The dispersion of a sample is known as the standard deviation, or `SD`, or sometimes just `s`.

-   The dispersion of a population cannot be a "deviation" because we don't actually know it. So instead, we call it the Standard Error, or SE, or most often (sigma).

    -   $SE = \frac{s}{\sqrt{n}}$

## Sampling theory, but candy

::::: columns
::: {.column width="50%"}
![](https://images.unsplash.com/photo-1534706013986-73f676db1790?w=900&auto=format&fit=crop&q=60&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxzZWFyY2h8N3x8Y2FuZHklMjBqYXJzfGVufDB8fDB8fHww)
:::

::: {.column width="50%"}
-   This is a jar of 600 pieces of candy.

<!-- -->

-   If I took scoops of 25 pieces of candy at a time, on average how many oranges would I get per scoop?

    -   I take a first scoop and 6 of them are orange.

-   Should I expect to get 6 oranges on average?

    -   Why?
:::
:::::

## Sampling theory, but candy

::::: columns
::: {.column width="50%"}
-   Did this scoop come from the jar of m&ms?

    -   Probably!

-   I have a scoop of 25 pieces of candy, and all 25 of them are orange.

-   Did this scoop come from the jar of m&ms?

    -   Probably not
:::

::: {.column width="50%"}
```{r}

# parameters
n <- 25
p_null <- 0.24
p_real <- 0.60
cutoff <- 25   # observed oranges in scoop

# x-axis: possible orange counts
x <- 0:n

# build distributions
df <- data.frame(
  x = rep(x, 2),
  prob = c(dbinom(x, n, p_null), dbinom(x, n, p_real)),
  dist = rep(c("p-null (mean ≈ 6)", "p-real (mean ≈ 15)"), each = length(x))
)

# highlight rejection region (p_null, x >= cutoff)
shade <- df %>%
  filter(dist == "p-null (mean ≈ 6)", x >= cutoff)

# plot
ggplot(df, aes(x, prob, color = dist)) +
  geom_line(size = 1.2) +
  geom_point(size = 2) +
  geom_col(
    data = shade,
    aes(x, prob),
    inherit.aes = FALSE,
    fill = "red",
    alpha = 0.4
  ) +
  scale_x_continuous(breaks = seq(0, n, 5)) +
  labs(
    x = "Number of Orange Candies per Scoop (n = 25)",
    y = "Probability",
    title = "Sampling Distributions with Rejection Region"
  ) +
  theme_minimal() + 
  theme_sub_legend(position = "bottom")
```
:::
:::::

## P-values

### Hypothesis Testing, Pt 1

## Sampling theory

> We ask every single FSC student to rate their sense of belonging on FSC campus on a scale of `1` (**I don't belong at all**) to `10` (**I belong completely**). We each calculate the average response from our own sample of `400`.

-   There are about `10,000` students at Farmingdale State College.

-   Each of the `25` of us recruits a sample of `400` students.

## Sampling theory

::::: columns
::: {.column width="50%"}
-   Now we have `25` samples of `400` FSC students each, which equals the full student population of `10,000`.

<!-- -->

-   We take `25` samples of `400` students each from another college...John Jacob Jinglehymer Smith University. JJJSMU also has `10,000` students.

-   We ask the JJJSMU students the same question about sense of belonging.

-   We find the mean of each of the `25` samples from JJJSMU
:::

::: {.column width="50%"}
```{r}

p
```
:::
:::::

## Sampling theory

## The Basis for Inferential Statistics

```{r}

library(ggplot2)
library(dplyr)

# parameters
n <- 500
sd_pop <- 3
se <- sd_pop / sqrt(n)   # standard error

mu_fsc <- 7   # "real" mean
mu_jsmu <- 4  # "null" mean

# x values
x <- seq(2, 9, length.out = 500)

# build distributions
df <- tibble(
  x = x,
  fsc = dnorm(x, mean = mu_fsc, sd = se),
  jsmu = dnorm(x, mean = mu_jsmu, sd = se)
) |> tidyr::pivot_longer(-x, names_to = "dist", values_to = "density")

# rejection cutoff (example)
cutoff <- 5.5

# plot
ggplot(df, aes(x, density, color = dist, lty = dist)) +
  geom_line(size = 1.2) +
  # geom_area(
  #   data = df |> filter(dist == "jsmu", x >= cutoff),
  #   aes(x, density),
  #   inherit.aes = FALSE,
  #   fill = "red",
  #   alpha = 0.4
  # ) +
  geom_vline(xintercept = cutoff, linetype = "dashed", color = "red") +
  annotate("text", x = cutoff + 0.1, y = max(df$density)/2,
           label = "Rejection Region", color = "red", hjust = 0) +
  theme_minimal() +
  labs(
    title = "Sampling Distributions with Rejection Region",
    x = "Sample Mean",
    y = "Density"
  )
```

## P-Real and P-Null

```{r}
library(patchwork)
prpn <- 
  tibble(
  z = seq(-4, 4, length.out = 1000),
  dx = dnorm(z)
  ) 

pr1 <- 
  prpn |> 
  ggplot(aes(x = z, y = dx)) +
  geom_line() +
  geom_vline(xintercept = 0, lty = "dashed") + 
  theme_void() +
  geom_area(data = filter(new_dat, z <= 2), fill = "darkorange", alpha = 0.6) + 
  labs(
  title = TeX("$H_0: \\mu_1 \\leq \\mu_2$")
  )

pr2 <- 
  prpn |> 
  ggplot(aes(x = z, y = dx)) +
  geom_line() +
  geom_vline(xintercept = 0, lty = "dashed") + 
  theme_void() +
  geom_area(data = filter(new_dat, z >= 2), fill = "steelblue", alpha = 0.6) +
   labs(
  title = TeX("$H_0: \\mu_1 > \\mu_2$")
  )

pr3 <- 
  prpn |> 
  ggplot(aes(x = z, y = dx)) +
  geom_line() +
  geom_vline(aes(xintercept = mean(z)), lty = "dashed") + 
  theme_void() +
  geom_area(data = filter(new_dat, z >= -2), fill = "darkorange", alpha = 0.6) + 
   labs(
  title = TeX("$H_0: \\mu_1 \\geq \\mu_2$")
  )

pr4 <- 
  prpn |> 
  ggplot(aes(x = z, y = dx)) +
  geom_line() +
  geom_vline(xintercept = 0, lty = "dashed") + 
  theme_void() +
  geom_area(data = filter(new_dat, z <= -2), fill = "steelblue", alpha = 0.6) +
   labs(
  title = TeX("$H_0: \\mu_1 < \\mu_2$")
  )

pr5 <- 
  prpn |> 
  ggplot(aes(x = z, y = dx)) +
  geom_line() +
  geom_vline(xintercept = 0, lty = "dashed") + 
  theme_void() +
  geom_area(data = filter(new_dat, z >=-1 & z <= 1), fill = "orange", alpha = 0.6) +
   labs(
  title = TeX("$H_0: \\mu_1 = \\mu_2$")
  )

pr6 <- 
  prpn |> 
  ggplot(aes(x = z, y = dx)) +
  geom_line() +
  geom_vline(xintercept = 0, lty = "dashed") + 
  theme_void() +
  geom_area(data = filter(new_dat, z <=-1), fill = "steelblue", alpha = 0.6) +
  geom_area(data = filter(new_dat, z >= 1), fill = "steelblue", alpha = 0.6) +
   labs(
  title = TeX("$H_0: \\mu_1  \neq \\mu_2$")
  )

pr1 + pr2 + pr3 + pr4 + pr5 + pr6 + plot_layout(ncol = 2, nrow = 3)

```

## What is a hypothesis?

A hypothesis is a testable prediction of what will happen in our experiment that:

-   Names of the variables (independent and dependent)

-   Clearly contrasts the groups

::: callout-important
## Remember!

Independent Variables are manipulated

Dependent Variables are measured
:::

## Hypothesis:

### Example

> Professor Brocker wants to know if Millennials enjoy the Netflix Original, *Dark* significantly more than Gen Z. He recruits 500 Millennials and 500 Gen-Z students and asks them to rate Dark on a scale of 1 to 10 (10 being fantastic).

-   Hypothesis: Millennials will rate their enjoyment of Dark as significantly higher than their Gen Z peers.

## What is a Hypothesis?

An **Alternative Hypothesis** is a testable prediction of what will happen in our experiment that:

-   Names of the variables (independent and dependent)

-   Clearly contrasts the groups.

<!-- -->

-   The Alternative Hypothesis is written as $H_1/H_A$

## Null Hypothesis

-   The Null Hypothesis states that nothing will happen. $H_0$

-   Because Null means zero, nothing, nada

<!-- -->

-   Names the variables (independent and dependent)

## Alternative & Null Hypotheses:

### Example: Professor Brocker's Dark Experiment:

::::: columns
::: {.column width="50%"}
-   Null Hypothesis: These is no difference in the DV between the IV groups.

-   Alternative Hypothesis: The experimental group is significantly different from the control group on the DV.
:::

::: {.column width="50%"}
-   **Alternative Hypothesis**: Millennials will rate their enjoyment of Dark as significantly higher than their Gen Z peers.

-   **Null Hypothesis**: Millennials and Gen Z will not differ in their rating of enjoyment of the Dark.
:::
:::::

## Hypothesis Testing

### Example 1

> Professor Brocker wants to know if giving his students coffee will improve their exam scores. He **randomly** assigns `13` of his `26` students to drink a doubleshot; he calls this the **experimental** group. The other `13` students drink decaf (a placebo); he calls this the **control** group.

$H_0$:

$H_1/H_A$:

## Hypothesis Testing

### Example 2

> Esmeralda gives an anti-depressant to `100` individuals suffering from depression. She gives another `100` individuals a placebo. After `2` months, we measure their depression.

$H_0$:

$H_1/H_A$:

## Hypothesis Testing

### Practice

> Jonas assigns *half* of the participants to engage in aerobic exercise for **one hour a day 5 days a week for 6 months**. The other half of the participants do not exercise for **6 months**. At the end of the 6 months, Jonas measures the participants' **working memory capacity**.

$H_0$:

$H_1/H_A$:

## Rejects and Failures

-   In science, we **do not** say that we proved anything.

    -   Nothing is ever really proven.

-   Our findings will be stated in terms of the Null Hypothesis H~0~.

-   The Null Hypothesis is that there are no differences between the groups.

## Hypothesis testing practice

> Jonas assigns *half* of the participants to engage in aerobic exercise for **one hour a day 5 days a week for 6 months**. The other half of the participants do not exercise for **6 months**. At the end of the 6 months, Jonas measures the participants' **working memory capacity**.

$H_0$: There are no differences in working memory capacity between the experimental and control groups.

$H_1/H_A$: Working memory capacity will be higher for participants who exercise for one hour compared to the group that does not.

## Rejects and Failures

-   In science, we do not say that we have proven anything.

-   Our findings will be stated in terms of the Null Hypothesis.

-   If there are **significant differences** between the groups, we **Reject the** $H_0$.

    -   Rejecting is 'good'.

-   If there are **NO differences** between the groups, we **Fail to Reject the** $H_0$.

    -   Failing is 'bad'

## Hypothesis Testing

When reporting your findings, you must state them in terms of the Null Hypothesis. We do not mention the Alternative Hypothesis.

-   If the groups differ, we reject the null hypothesis:

    -   **Reject** $H_0$.

-   If the groups do not differ, we fail:

    -   **Fail to Reject** $H_0$

## Practice

> Jonas assigns half of the participants to engage in aerobic exercise for one hour a day `5` days a week for `6` months. The other half of the participants do not exercise for `6` months. At the end of the `6` months, Jonas measures the participants' working memory capacity. His test is not significant.

-   State Jonas' findings in terms of the null hypothesis:
-   *Jonas fails to reject the null hypothesis that there is no difference between the experimental and control group in regards to exercising and its effect on working memory*

## Hypothesis Practice

### Practice

> Brendan assigns half of the participants to view a picture of a face on a mortuary table (control condition). The other half of the participants view an image of their own face made to look dead using an filter (experimental condition). Brendan then measures all participants' anxiety about dying and runs a t-test, which is statistically significant.

-   State Brendan's findings in terms of the null hypothesis:

-   *Brendan rejects the null hypothesis that states there is no difference in death anxiety betwen participants who view their own face as deceased and those who only view a face on a mortuary table.*

## Practice Problem

::: {#example-box}
> John scores an `87` on a test. If the SD is `2` and the mean is `90`, what percentage scored higher than John?
:::

<button onclick="newExample()">

New Example

</button>

```{=html}
<script>
const names = ["John","Aisha","Miguel","Sofia","Ethan","Priya","Liam","Zoe","Kai","Amira"];
const contexts = [
  "math test","running race","psychology exam","biology quiz",
  "typing speed challenge","chemistry final","geography test",
  "coding challenge","swimming competition","history pop quiz"
];

function newExample(){
  // pick random name and context
  const name = names[Math.floor(Math.random() * names.length)];
  const context = contexts[Math.floor(Math.random() * contexts.length)];

  // pick random mean and sd
  const mean = Math.floor(Math.random() * (95 - 70 + 1)) + 70; // 70–95
  const sd = Math.floor(Math.random() * (6 - 2 + 1)) + 2;      // 2–6
  // pick score close to mean
  const score = Math.floor(Math.random() * (mean + 10 - (mean - 10) + 1)) + (mean - 10);

  document.getElementById("example-box").innerHTML =
    `<blockquote>${name} scores <code>${score}</code> on a ${context}. 
     If the SD is <code>${sd}</code> and the mean is <code>${mean}</code>,
     what percentage scored higher than ${name}?</blockquote>`;
}
</script>
```
