---
title: "Sampling Theory"
subtitle: "Lecture 7"
author: "Dave Brocker"
institute: "Farmingdale State College"
format: 
  revealjs:
    theme: custom.scss
    width: 1600
    height: 900
    incremental: true   
    touch: true
    chalkboard: true
filters:
  - webr
---

## Standard Normal Distribution

The 4 properties of a standard normal distribution are:

::::: columns
::: {.column width="50%"}
```{r}

library(ggplot2)
library(dplyr)

ggnorm <- function(n, m, sd) {
  x <- rnorm(n, m, sd)
  tibble(
    x = x,
    pd = dnorm(x, mean(x), sd(x))
  ) |>
    ggplot(aes(x, pd)) +
    geom_line() +
    geom_vline(aes(xintercept = mean(x)), 
               lty = "dashed") +
    theme_minimal() +
    scale_x_continuous(
      breaks = scales::pretty_breaks(n = 8)
      ) +
    labs(
      x = "\nValues",
      y = "Probability Density\n"
    )
}

ggnorm(1000,40,3)
```
:::

::: {.column width="50%"}
-   They are shaped like a bell ("bell curve").

-   They are symmetric.

-   They are unimodal.

-   The mean = median = mode.
:::
:::::

## Statistics

-   Descriptive Statistics:

    -   Goal: Describe the sample

        -   Examples: Mean, Standard Deviation

-   Inferential Statistics:

    -   Goal: Use the sample to make inferences about the population

        -   t-test
        -   ANOVA
        -   Regression

## Sampling Theory

> Professor Brocker *still* wants to know how much adults in the US enjoy the Netflix Original, *Dark*. He has unlimited funds to study this **very** important research question. He hires 30 students to collect the data. Each student has to collect 500 responses to the following question:

## Sampling Theory

> On a scale of 1 (I hate it with my entire being) to 10 (I believe in my soul that Dark is the best show ever made), how much do you enjoy Dark?

![](images/dark.png){style="border-radius: 15px;" fig-align="center" width="419"}

## Sampling Theory

::::: columns
::: {.column width="50%"}
-   Each student asks 500 people how much they enjoy the Netflix Original Dark.

-   How many people did each of you ask?

    -   $30 \times 500 = 15,000$
:::

::: {.column width="50%"}
```{r}
#| code-fold: true
#| echo: true
library(dplyr)
library(tibble)
library(gt)
library(latex2exp)
# 30 Students in the class
n = 30

# Each student asks 500 people
full_sample = n*500

# Simulate Results
replicate(n = 30,
            rnorm(500,5,7)) |> round(0) |> 
  data.frame() |> 
  rename_with(.fn = function(x) paste0("S", 1:30)) |> 
  summarize_all(mean) |>
  tidyr::pivot_longer(cols = everything(),
                      names_to = "Sample",
                      values_to = "Sample Mean") |> 
  gt() |> 
  tab_options(
    table.width = pct(60),
    table.font.size = 30,
    ) |> 
  opt_row_striping() |> 
  opt_interactive()
```
:::
:::::

## Sampling Theory

-   Once each of you collect *500 responses*, I ask you to calculate the average answer. So you calculate the mean.

::: fragment
-   $\bar{x}_1 = \frac{\sum(x_1 + x_2 + x_3+x_4...+ x_{500)}}{500}$

-   They are the means from each of your samples.

-   These are called **Sample Means**
:::

## Sampling Theory

### Here is our Ratings Data

```{r}
#| code-fold: true
library(ggplot2)
library(forcats)
library(gganimate)

tibble(
  ratings = rnorm(15000,6.6,2),
    dx = dnorm(ratings,mean(ratings),sd(ratings))
  ) |> 
  ggplot(aes(ratings, dx)) +
  geom_line() + 
  theme_minimal(base_size = 20) +
  labs(
    y = "Probability Density\n",
    x = "\nAverage Ratings for Dark"
  ) +
  geom_vline(aes(xintercept = mean(ratings)), lty = "dashed") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) + 
  theme_sub_panel(grid = element_blank())


```

## Sampling theory

### What can we do with Sample Means?

-   We could calculate the Mean of the Sample Means.

-   We could calculate the standard deviation of the Sample Means.

-   But...it's theoretical. It doesn't really exist, but we imagine it's existence for the sake of **Sampling Theory**.

## Sampling Distribution

::::: columns
::: {.column width="50%"}
-   A **Sampling Distribution** is the theoretical distribution of means across every single possible sample.

-   If we took samples of the population until we got every single person in the population, then calculated the mean for each sample, they would distribute like this:
:::

::: {.column width="50%"}
```{r}
tibble(
  ratings = rnorm(15000,6.6,2),
    dx = dnorm(ratings,mean(ratings),sd(ratings))
  ) |> 
  ggplot(aes(ratings, dx)) +
  geom_line() + 
  theme_minimal(base_size = 20) +
  labs(
    y = "Probability Density\n",
    x = "\nAverage Ratings for Dark"
  ) +
  geom_vline(aes(xintercept = mean(ratings)), lty = "dashed") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 8))
```
:::
:::::

## Sampling Theory

::::: columns
::: {.column width="50%"}
-   Except now each value in this distribution no longer represents 1 person/participant.

-   Each value in this distribution represents the average of 1 sample, a Sample Mean.

    -   In our case, each value is the average of `30` samples of `500` people.
:::

::: {.column width="50%"}
```{r}

new_dat <- tibble(
  z = seq(-4, 4, length.out = 1000),
  dx = dnorm(z)
)

labels <- tibble(
  x = c(-1, 1, -1.5, 1.5, -2.6, 2.6, -3.5, 3.5),
  y = c(.36,0.36, 0.25, 0.25, 0.075, 0.075, 0.025, 0.025),
  label = c("34.1%", "34.1%", "13.6%", "13.6%", "2.1%", "2.1%", "0.1%", "0.1%")
)


p <- 
  ggplot(new_dat, aes(x = z, y = dx)) +
  geom_line(size = 1.2) +
  geom_area(data = filter(new_dat, z >= -1 & z <= 1), fill = "lightblue", alpha = 0.6) +
  geom_area(data = filter(new_dat, z >=-1 & z <=0), fill = "lightblue", alpha = .6) +
  geom_area(data = filter(new_dat, z >=-2 & z <=-1), fill = "blue", alpha = .6) +
  geom_area(data = filter(new_dat, z <=2 & z >=1), fill = "blue", alpha = .6) +
  geom_area(data = filter(new_dat, z >=-4 & z <= -2), fill = "darkblue", alpha = .6) +
  geom_area(data = filter(new_dat, z >=2 & z <= 4), fill = "darkblue", alpha = .6) +
  geom_text(data = labels, aes(x = x, y = y, label = label), size = 4, inherit.aes = FALSE) +
  theme_minimal(base_size = 13) +
  labs(
    x = "\nStandard Deviations",
    y = "Probability Density\n"
  ) +
  scale_x_continuous(breaks = -4:4) +
  theme_sub_panel(grid = element_blank())

p
```
:::
:::::

## New Terminology

### Sample Mean and Sample Standard Deviation

::::: columns
::: {.column width="50%"}
-   We previously referred to the average as $\bar{x}$ and the standard deviation as $s$

-   The new terms refer to their specific attributes as a statistic of the sample

-   $\mu$ (pronounced like 'mew') represents the population mean

-   $\sigma$ represents the population standard deviation
:::

::: {.column width="50%"}
```{r}

library(tibble)

tibble(
  Term = c(
    "Sample Mean",
    "Sample SD",
    "Population Mean",
    "Population SD"
  ),
  Symbol = c(
    "$$\\bar{x}$$",
    "$$s$$",
    "$$\\mu$$",
    "$$\\sigma$$"
  ),
  Type = c(
    "Sample",
    "Sample",
    "Population",
    "Population"
  ),
  Formula = c(
    "$$\\frac{\\sum x_i}{n}$$",
    "$$\\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n-1}}$$",
    "$$\\frac{\\sum X_i}{N}$$",
    "$$\\sqrt{\\frac{\\sum (X_i - \\mu)^2}{N}}$$"
  ),
  Relation_to_Sample = c(
    "Mean of observed sample",
    "SD of sample values",
    "True (unknown) mean",
    "True (unknown) SD"
  )
) |> 
  gt() |> 
  fmt_markdown(columns = c(Formula,Symbol)) |> 
  opt_row_striping() |> 
  cols_label(
     "Relation_to_Sample" = "Relation to Sample"
  ) |> 
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  )

```
:::
:::::

## Sampling Theory

-   Each value in this distribution no longer represents 1 person/participant.

-   Each value in this distribution now represents the average of 1 sample, a Sample Mean.

```{r}

tibble(
  ratings = rnorm(15000,6.6,2),
    dx = dnorm(ratings,mean(ratings),sd(ratings))
  ) |> 
  ggplot(aes(ratings, dx)) +
  geom_line() + 
  theme_minimal(base_size = 20) +
  labs(
    y = "Probability Density\n",
    x = "\nAverage Ratings for Dark"
  ) +
  geom_vline(aes(xintercept = mean(ratings)), lty = "dashed") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +
  annotate(geom = "point", x = 3, y = .04, color = "maroon") +
  geom_segment(x = 3, y = 0, xend = 3, yend = .036, size = .8) +
  theme_sub_panel(grid = element_blank())
```

## Probability

-   What percentage of participants rated Dark with a z-score of 2 or higher?

```{r}
p
```

## Probability

-   What percentage of participants rated Dark with a z-score of 2 higher?

```{r}
p + 
  geom_segment(x = 2, xend = 2, y = 0, yend = .05) +
  geom_area(data = p$data |> filter(z <= 2), fill = "white")
```

## Probability

-   This is the distribution of **sample means** from adults in the US.

-   What is the probability of any sample having a mean that is a z-score of 2 or higher?

```{r}
p + 
  geom_segment(x = 2, xend = 2, y = 0, yend = .05) +
  geom_area(data = p$data |> filter(z <= 2), fill = "white")
```

## Assumptions of the Normal Curve

-   We *assume* that the distribution of sample means is normal.

-   We use that assumption to gauge the probability of getting a particular mean from a single sample.

-   We can literally find that probability, the same way we did with x-values in a normal distribution.

## Example

### Let Me Get Uhh....

We want to know how much people like pizza.

-   There are `12,500` people in our population. Each of the `25` of us collects a sample of `500`.

-   $500 \times 25 = 12,500$

-   Each of us calculates the mean response from our sample of `500` people.

-   What do we call those means?

    -   What would it look like if we were to plot it?

## Pizza Plot

### Visual Example

```{r}
set.seed(123)

df <- data.frame(
  sample_mean = replicate(
    10000,
    mean(rnorm(500, 4.2, 1))
  )
)

ggplot(df, aes(sample_mean)) +
  geom_histogram(
    aes(y = after_stat(density)),   # density instead of counts
    fill = "coral",
    bins = 40,
    alpha = 0.6
  ) +
  stat_function(
    fun = dnorm,
    args = list(
      mean = 4.2,
      sd = 1/sqrt(500)      # theoretical SE
    ),
    color = "blue",
    size = 1
  ) +
  theme_minimal() +
  labs(
    x = "\nSample Mean",
    y = "Density\n",
    title = "Sampling Distribution of the Mean (n = 500)"
  )
  
```

## Example

-   I randomly choose a mean from our distribution of sample means, about how much people like pizza.

-   What is the probability of picking a mean with a z-score of -1 or less?

```{r}
p
```

## Example

-   I randomly choose a mean from our distribution of sample means, about how much people like pizza.

-   What is the probability of picking a mean with a z-score of -1 or less?

```{r}
p + 
  geom_area(data = p$data |> filter(z>=-1), fill = "white")
```

## Sampling Theory

> Sampling Theory is the body of principles underlying the drawing of infinite samples that accurately represent the population from which they are taken and to which inferences can be made.

-   The distribution made up of sample means is **conceptual** and **theoretical**

## Sampling Theory

> Sampling Theory is the idea that if we took infinite samples of a population, they would create a normal distribution.

-   If that is correct, we can then make guesses about the probability of getting a specific mean.

-   We will use this probability to gauge **significance** of our inferential statistics.

## Sampling Theory

### New Terminology (Again???)

-   The mean of a sample is notated as $M$.

-   The mean of a population is notated as ($\mu$).

-   The dispersion of a sample is known as the standard deviation, or $SD$, or sometimes just $s$.

## Standard Error

### A Population Deviation

-   $SE = \frac{\sigma}{\sqrt{n}}$

    -   $\sigma$ = Population Standard Deviation

    -   $n$ - Sample Size

-   The dispersion of a population cannot be a "deviation" because we don't actually know it. So instead, we call it the Standard Error, or SE, or most often (sigma).

## Sampling theory, but candy

::::: columns
::: {.column width="50%"}
![](images/candyjar.png){style="border-radius: 20px;"}
:::

::: {.column width="50%"}
-   This is a jar of `500` pieces of candy. There are `5` flavors of candy.

-   If I took scoops of `25` pieces of candy at a time, on average how many oranges would I get per scoop?

    -   I take a first scoop and 5 of them are orange.

    -   Should I expect to get 5 oranges on average?
:::
:::::

## Sampling Theory

### With Candy!

::::: columns
::: {.column width="50%"}
-   Is it safe to assume the `5`-Orange scoop came from the Candy Jar?

    -   Probably!

-   I take a scoop of `25` pieces of candy, and all `25` of them are orange.

-   Did this scoop come from the Candy Jar?

    -   Probably not
:::

::: {.column width="50%"}
```{r}
set.seed(123)
library(purrr)

jar <- rep(c("Orange","Green","Red","Yellow","Purple"), each = 100)

tibble(
  scoop = factor(1:20),
  n_orange = map_int(1:20, ~ sum(sample(jar, 25, replace = FALSE) == "Orange"))
) |> 
  ggplot(aes(n_orange,scoop)) +
  geom_col(fill = "orange", color = "black", width = 0.9) +
  theme_minimal() +
  labs(
    x = "\nNumber of Oranges in Scoop",
    y = "Scoop Number\n",
    title = "Distribution of Oranges in 20 Scoops of 25"
  ) + 
  geom_vline(aes(xintercept = mean(n_orange)), lty = "dashed") + 
  theme_sub_panel(grid.major = element_blank())
```
:::
:::::

## Null Hypothesis Significance Testing

### NHST

A hypothesis is a testable prediction of what will happen in our experiment that:

-   Names of the variables (independent and dependent)

-   Clearly contrasts the groups

## NHST

### Null Hypothesis

-   The Null Hypothesis states that nothing will happen or that there is no difference between. $H_0$

-   Null means zero, nothing

-   Names the variables (independent and dependent)

-   Indicates or suggests no difference between the groups

    -   $\mu_1 = \mu_2$

## NHST

### Alternative Hypothesis

An **Alternative Hypothesis** is a testable prediction of what will happen in our experiment that:

-   Names the variables (independent and dependent)

-   Clearly contrasts the groups.

-   The Alternative Hypothesis is written as $H_1/H_A$

    -   $\mu_1 > \mu_2$

## **The Question**

> Every year we run a huge campus climate survey. We can’t ask all 10,000 students, so we draw a sample. Now imagine each of *you* is in charge of your own survey, each collecting 400 students’ responses.

-   FSC has **10,000 students**.

-   Asking all 10,000 is not practical.

-   Each of the **25 of us** collects a **sample of 400 students**.

-   We calculate the **average belonging score** in each sample.

    -   These averages are called **sample means**.

## **Sampling Variation**

::::: columns
::: {.column width="50%"}
-   Sample means will **not be identical**.

-   Why?

    -   Different mixes of students (e.g., commuters, first-years, club leaders).

-   This variation from sample to sample is called **sampling variation**.

    -   It’s the foundation of **sampling theory**.
:::

::: {.column width="50%"}
```{r}
set.seed(123)
sample_means <- 
  tibble(
    mean = 
      replicate(25, 
                mean(runif(400,1,9))
      )
)  

ggplot(sample_means, aes(x = mean)) +
  geom_histogram(color = "black", fill = "skyblue", bins = 10) +
  labs(
    title = "Sampling Variability of the Sample Mean",
    x = "\nSample Mean",
    y = "Frequency\n",
    subtitle = "We don’t get the exact same answer each time — but the sample means cluster around the true mean."
  ) +
  theme_minimal() + 
  theme_sub_panel(grid.major = element_blank())

```
:::
:::::

## Sampling Distribution

-   If we plot all **25 sample means** from FSC:

    -   Most cluster near the **true population mean**.

    -   Some fall higher or lower by chance.

-   This plot is the **sampling distribution of the mean**.

-   As samples get larger, the sampling distribution becomes **narrower**.

## **Comparing Two Schools**

> We want to compare FSC students’ sense of belonging to that at another college. We draw 25 samples of 400 at each school.

-   We repeat the study at **John Jacob Jinglehymer Smith University (JJJSMU)**.

-   We now have **two sampling distributions**:

    -   One for FSC

    -   One for JJJSMU

## Comparing Two Schools

::::: columns
::: {.column width="50%"}
-   Comparing the two distributions:

    -   Differences in the **centers** suggest differences in population belonging.

    -   Differences in the **spread** reflect sampling error.
:::

::: {.column width="50%"}
```{r}
set.seed(101)
df_sampling <- tibble(
  value = c(rnorm(500, mean = 4.4, sd = 1.2),
            rnorm(500, mean = 8.8, sd = 1.2)),
  group = rep(c("FSC","JJJSMU"), each = 500)
)

df_sampling |> 
  ggplot(aes(x = value, fill = group)) +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = c(4.4,8.8),lty = "dashed") +
  labs(title = "Two Overlapping Distributions as Population Distributions",
       x = "\nValue", y = "Density\n") +
  theme_minimal()
```
:::
:::::

## Comparing Two Schools

### Two Distributions

```{r}
set.seed(101)
df_sampling <- tibble(
  value = c(rnorm(500, mean = 4.4, sd = 1.2),
            rnorm(500, mean = 8.8, sd = 1.2)),
  group = rep(c("FSC","JJJSMU"), each = 500)
)

df_sampling |> 
  ggplot(aes(x = value, fill = group)) +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = c(4.4,8.8),lty = "dashed") +
  labs(title = "Two Overlapping Distributions as Population Distributions",
       x = "\nValue", y = "Density\n") +
  theme_minimal(paper = "cornsilk",ink = "navy")
```

## **Why It Matters**

### Sampling distributions let us:

-   Estimate population parameters.

    -   What is the mean 'likely' to be?

-   Understand and quantify **sampling error**.

    -   How can we explain sampling variation

-   Compare populations in a statistically rigorous way.

    -   Did a datapoint (or statistics) come from the same population?

## Probability

### A Refresher

```{r}
library(gganimate)
library(stringr)
coin <- c("H","T")
prob <- c(.5,.5)

map(1:1000, ~sample(coin, size = .x, prob = prob, replace = TRUE)) |>
  tibble(flip = _) |>
  mutate(
    hf = str_count(flip, "H") / row_number(flip),
    num = row_number()
  ) |>
  ggplot(aes(num, hf)) +
  geom_line() +
  theme_minimal() +
  labs(
    x = "\nNumber of Coin Flips",
    y = "Proportion of Heads\n"
  ) +
  transition_reveal(num)
```

## P-Real and P-Null

### Alternative and Null

```{r}
library(patchwork)
prpn <- 
  tibble(
  z = seq(-4, 4, length.out = 1000),
  dx = dnorm(z)
  ) 

pr1 <- 
  prpn |> 
  ggplot(aes(x = z, y = dx)) +
  geom_line() +
  geom_vline(xintercept = 0, lty = "dashed") + 
  theme_void() +
  geom_area(data = filter(new_dat, z <= 2), fill = "darkorange", alpha = 0.6) + 
  labs(
  title = TeX("$H_0: \\mu_1 \\leq \\mu_2$")
  )

pr2 <- 
  prpn |> 
  ggplot(aes(x = z, y = dx)) +
  geom_line() +
  geom_vline(xintercept = 0, lty = "dashed") + 
  theme_void() +
  geom_area(data = filter(new_dat, z >= 2), fill = "steelblue", alpha = 0.6) +
   labs(
  title = TeX("$H_A: \\mu_1 > \\mu_2$")
  )

pr3 <- 
  prpn |> 
  ggplot(aes(x = z, y = dx)) +
  geom_line() +
  geom_vline(aes(xintercept = mean(z)), lty = "dashed") + 
  theme_void() +
  geom_area(data = filter(new_dat, z >= -2), fill = "darkorange", alpha = 0.6) + 
   labs(
  title = TeX("$H_0: \\mu_1 \\geq \\mu_2$")
  )

pr4 <- 
  prpn |> 
  ggplot(aes(x = z, y = dx)) +
  geom_line() +
  geom_vline(xintercept = 0, lty = "dashed") + 
  theme_void() +
  geom_area(data = filter(new_dat, z <= -2), fill = "steelblue", alpha = 0.6) +
   labs(
  title = TeX("$H_A: \\mu_1 < \\mu_2$")
  )

pr5 <- 
  prpn |> 
  ggplot(aes(x = z, y = dx)) +
  geom_line() +
  geom_vline(xintercept = 0, lty = "dashed") + 
  theme_void() +
  geom_area(data = filter(new_dat, z >=-1 & z <= 1), fill = "orange", alpha = 0.6) +
   labs(
  title = TeX("$H_0: \\mu_1 = \\mu_2$")
  )

pr6 <- 
  prpn |> 
  ggplot(aes(x = z, y = dx)) +
  geom_line() +
  geom_vline(xintercept = 0, lty = "dashed") + 
  theme_void() +
  geom_area(data = filter(new_dat, z <=-1), fill = "steelblue", alpha = 0.6) +
  geom_area(data = filter(new_dat, z >= 1), fill = "steelblue", alpha = 0.6) +
   labs(
  title = TeX("$H_A: \\mu_1  \\neq \\mu_2$")
  )

pr1 + pr2 + pr3 + pr4 + pr5 + pr6 + 
  plot_layout(ncol = 2, nrow = 3) + 
  plot_annotation(
    title = "Null Hypothesis and Alternative Hypothesis", 
    theme = theme(title = element_text(size = 15))
    )

```

## The Lady Tasting Tea

### The Story

:::::: columns
:::: {.column width="50%"}
::: quote
A lady declares that by tasting a cup of tea made with milk she can discriminate whether the milk or the tea infusion was first added to the cup. We will consider the problem of designing an experiment by means of which this assertion can be tested. \[…\] \[It\] consists in mixing eight cups of tea, four in one way and four in the other, and presenting them to the subject for judgment in a random order. The subject has been told in advance of that the test will consist, namely, that she will be asked to taste eight cups, that these shall be four of each kind \[…\]. — Fisher, 1935.
:::
::::

::: {.column width="50%"}
![](images/teaexperiment.png){style="border-radius: 30px;"}
:::
::::::

## The Lady Tasting Tea

### The Story

::::: columns
::: {.column width="50%"}
-   A lady claimed she could tell whether **milk or tea was poured first**.
-   Fisher designed an experiment:
    -   8 cups of tea
    -   4 poured milk-first, 4 tea-first
    -   **Randomized order**
-   The lady must classify each cup.
:::

::: {.column width="50%"}
![](images/teaexperiment.png)

A classic example of hypothesis testing\
*Based on R.A. Fisher’s 1935 design*
:::
:::::

## The Hypotheses

-   **Null hypothesis (H₀):**

    -   The lady has **no special ability** — she is just guessing.

-   **Alternative (H₁):**

    -   The lady **can tell the difference**.

-   **Key idea:**

    -   We begin by assuming H₀ is true.

    -   We’ll only reject H₀ if the data are *too surprising* under H₀.

## Possible Outcomes

We focus on **how many cups she classifies correctly**.

For 8 cups (4 of each), under H₀:

::::: columns
::: {.column width="50%"}
![](images/teachoices.png)
:::

::: {.column width="50%"}
```{r}
tibble(
  Correct = c(8, 7, 6, 5, 4),
  Probability = c("1 / 70 ≈ 1.4%", "16 / 70 ≈ 22.9%", "36 / 70 ≈ 51.4%", "16 / 70 ≈ 22.9%", "1 / 70 ≈ 1.4%")
) %>% 
  gt() |> 
  opt_row_striping() |> 
  tab_options(
    table.width = pct(80),table.font.size = 22
    )
```
:::
:::::

## The Test

-   Fisher’s original rule: **Reject H₀ only if she gets all 8 correct.**
-   Under H₀: ${8 \choose 4} = \frac{8!}{4!(8-4)} = 70$

Since this is less than 5%, it’s rare enough to count as evidence against H₀.

## Decision & Interpretation

-   **If 8 / 8 correct → Reject H₀**

    -   Suggests evidence she can tell the difference.\

-   **Otherwise → Fail to reject H₀**

    -   No convincing evidence of ability.

. . .

> We never *prove* the alternative; we only judge whether the data are too unlikely under the null.

## The Logic

1.  Assume a **null model**.
2.  Compute how likely the observed data are under that model.
3.  If that probability (p-value) is very small → reject the null.

-   This logic is the same for t-tests, z-tests, and many modern methods.

```{r}
#| echo: true
#| code-fold: false

# Probability of getting all 8 correct under H₀ 
k <- 0:4 

dhyper(k, m = 4, n = 4, k = 4)
```

## Hypothesis:

### Example

> Professor Brocker wants to know if Millennials enjoy the Netflix Original, *Dark* significantly more than Gen Z. He recruits 500 Millennials and 500 Gen-Z students and asks them to rate Dark on a scale of 1 to 10 (10 being fantastic).

-   Hypothesis: Millennials will rate their enjoyment of Dark as significantly higher than their Gen Z peers.

## Hypothesis:

### Example

> Professor Brocker wants to know if Millennials enjoy the Netflix Original, *Dark* significantly more than Gen Z. He recruits 500 Millennials and 500 Gen-Z students and asks them to rate Dark on a scale of 1 to 10 (10 being fantastic).

-   Null Hypothesis: These is no difference in the DV between the IV groups.

-   Alternative Hypothesis: The experimental group is significantly different from the control group on the DV.

## Hypothesis:

### Example

> Professor Brocker wants to know if Millennials enjoy the Netflix Original, *Dark* significantly more than Gen Z. He recruits 500 Millennials and 500 Gen-Z students and asks them to rate Dark on a scale of 1 to 10 (10 being fantastic).

-   **Alternative Hypothesis**: Millennials will rate their enjoyment of Dark as significantly higher than their Gen Z peers.

-   **Null Hypothesis**: Millennials and Gen Z will not differ in their rating of enjoyment of the Dark.

## Hypothesis Testing

### Example 1

> Professor Brocker wants to know if giving his students coffee will improve their exam scores. He **randomly** assigns `13` of his `26` students to drink a doubleshot; he calls this the **experimental** group. The other `13` students drink decaf (a placebo); he calls this the **control** group.

-   $H_0$:

    -   $\mu_\text{Coffee} = \mu_\text{Decaf}$

-   $H_1/H_A$:

    -   $\mu_{\text{Coffee}} > \mu_{\text{Decaf}}$

## Hypothesis Testing

### Example 2

> Esmeralda gives an anti-depressant to `100` individuals suffering from depression. She gives another `100` individuals a placebo. After `2` months, we measure their depression.

-   $H_0$:

    -   $\mu_{\text{SSRI}} = \mu_{\text{Decaf}}$

-   $H_1/H_A$:

    -   $\mu_{\text{SSRI}} < \mu_{\text{Placebo}}$

## Hypothesis Testing

### Practice

> Jonas assigns *half* of the participants to engage in aerobic exercise for **one hour a day for 6 months**. The other half of the participants do not exercise for **6 months**. At the end of the 6 months, Jonas measures the participants' **working memory capacity**.

-   $H_0$:

    -   $\mu_{\text{Exercise}} = \mu_{\text{No Exercise}}$

-   $H_1/H_A$:

    -   $\mu_{\text{Exercise}} > \mu_{\text{No Exercise}}$

## Rejects and Failures

-   In science, we **do not** say that we proved anything.

    -   Nothing is ever really proven.

-   Our findings will be stated in terms of the Null Hypothesis $H_0$.

-   The Null Hypothesis is that there are no differences between the groups.

## Hypothesis testing practice

> Jonas assigns *half* of the participants to engage in aerobic exercise for **one hour a day 5 days a week for 6 months**. The other half of the participants do not exercise for **6 months**. At the end of the 6 months, Jonas measures the participants' **working memory capacity**.

-   $H_0$: There are no differences in working memory capacity between the experimental and control groups.

-   $H_1/H_A$: Working memory capacity will be higher for participants who exercise for one hour compared to the group that does not.

## Rejects and Failures

-   In science, we do not say that we have proven anything.

-   Our findings will be stated in terms of the Null Hypothesis.

-   If there are **significant differences** between the groups, we **Reject the** $H_0$.

    -   Rejecting is 'good'.

-   If there are **No differences** between the groups, we **Fail to Reject the** $H_0$.

    -   Failing is 'bad'

## Hypothesis Testing

When reporting your findings, you must state them in terms of the Null Hypothesis. We do not mention the Alternative Hypothesis.

-   If the groups differ, we reject the null hypothesis:

    -   **Reject** $H_0$.

-   If the groups do not differ, we fail:

    -   **Fail to Reject** $H_0$

## Practice

> Jonas assigns half of the participants to engage in aerobic exercise for one hour a day `5` days a week for `6` months. The other half of the participants do not exercise for `6` months. At the end of the `6` months, Jonas measures the participants' working memory capacity. His test is not significant.

-   State Jonas' findings in terms of the null hypothesis:
-   *Jonas fails to reject the null hypothesis that there is no difference between the experimental and control group in regards to exercising and its effect on working memory*

## Hypothesis Practice

### Practice

> Brendan assigns half of the participants to view a picture of a face on a mortuary table (control condition). The other half of the participants view an image of their own face made to look dead using an filter (experimental condition). Brendan then measures all participants' anxiety about dying and runs a t-test, which is statistically significant.

-   State Brendan's findings in terms of the null hypothesis:

-   *Brendan rejects the null hypothesis that states there is no difference in death anxiety betwen participants who view their own face as deceased and those who only view a face on a mortuary table.*

<!-- ## Practice Problem -->

<!-- ::: {#example-box} -->

<!-- > John scores an `87` on a test. If the SD is `2` and the mean is `90`, what percentage scored higher than John? -->

<!-- ::: -->

<!-- <button onclick="newExample()"> -->

<!-- New Example -->

<!-- </button> -->

<!-- ```{=html} -->

<!-- <script> -->

<!-- const names = ["John","Aisha","Miguel","Sofia","Ethan","Priya","Liam","Zoe","Kai","Amira"]; -->

<!-- const contexts = [ -->

<!--   "math test","running race","psychology exam","biology quiz", -->

<!--   "typing speed challenge","chemistry final","geography test", -->

<!--   "coding challenge","swimming competition","history pop quiz" -->

<!-- ]; -->

<!-- function newExample(){ -->

<!--   // pick random name and context -->

<!--   const name = names[Math.floor(Math.random() * names.length)]; -->

<!--   const context = contexts[Math.floor(Math.random() * contexts.length)]; -->

<!--   // pick random mean and sd -->

<!--   const mean = Math.floor(Math.random() * (95 - 70 + 1)) + 70; // 70–95 -->

<!--   const sd = Math.floor(Math.random() * (6 - 2 + 1)) + 2;      // 2–6 -->

<!--   // pick score close to mean -->

<!--   const score = Math.floor(Math.random() * (mean + 10 - (mean - 10) + 1)) + (mean - 10); -->

<!--   document.getElementById("example-box").innerHTML = -->

<!--     `<blockquote>${name} scores <code>${score}</code> on a ${context}.  -->

<!--      If the SD is <code>${sd}</code> and the mean is <code>${mean}</code>, -->

<!--      what percentage scored higher than ${name}?</blockquote>`; -->

<!-- } -->

<!-- </script> -->

<!-- ``` -->
