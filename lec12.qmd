---
title: "Linear Regression"
subtitle: "Lecture 11"
author: "Dave Brocker"
footer: "⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢"
institute: "Farmingdale State College"
format: 
  revealjs:
    theme: custom.scss
    scrollable: true
    incremental: true   
    touch: true
    chalkboard: true
    lightbox: true
    code-fold: true
    drop:
      engine: webr
      webr:
        packages:
         - ggplot2
         - dplyr
revealjs-plugins:
  - drop
---

## Types of Analysis

-   **Univariate**: One variable, like the mean

-   **Bivariate**: Two variables, like correlation

-   **Multivariate**: More than 2 variables

## Regression

-   Regression is the **multivariate** version of correlation.

-   Correlation is the **bivariate** version of regression.

-   We're doing the same thing in regression as we do in correlation, **BUT** there are more than 2 variables.

## Regression terminology

In regression we will choose one variable to predict.

-   We call this $Y$.

-   $Y$ is the outcome variable.

-   $\hat{Y}$ is the predicted variable.

## Regression terminology

In regression we will choose two or more variables we have reason to believe predict y.

-   We call these $A$, $B$, etc.

-   \$A\$ is the predictor variable.

## Regression terminology

When we predict Y with `X`, we say;

We regress `y` on the `x`.

## Regression Equation

### Step-by-Step

$$
\large{\color{red}{Y_i} = \color{green}{\beta_0} + \color{blue}{\beta_1(A)}+\color{orange}{e_i}}
$$

::: fragment
[1. The predicted value of y for a particular participant (i)]{style="color:red"}
:::

::: fragment
[2. Intercept]{style="color:green"}: Value of Y when all other predictors are 0
:::

::: fragment
[3. Weight]{style="color:blue"}
:::

::: fragment
[4. The Value of a particular participant on the measure of $A$]{style="color:black"}
:::

::: fragment
[5. Error. Random, and therefore cannot be predicted]{style="color:orange"}
:::

## Regression Equation

### A Few Ways to Write the Same Thing!

$$
\large{\color{red}{Y_i} = \color{green}{\beta_0} + \color{blue}{\beta_1(A)}}
$$

::: fragment
$$\hat{Y} = \beta_0 + \beta_1(A)$$
:::

::: fragment
$$Y = ax + b$$
:::

## Linear Regression

### Line of Best Fit

```{r}
library(dplyr)
library(ggplot2)
library(MASS)
library(gtsummary)

generate_correlation <- function(n, rho) {
  # Define the mean and covariance matrix
  mean_vector <- c(0, 0)
  covariance_matrix <- matrix(c(1, rho, rho, 1), nrow = 2)
  
  # Generate correlated data using mvrnorm from the MASS package
  data <- mvrnorm(n = n, mu = mean_vector, Sigma = covariance_matrix)
  
  # Convert to a data frame
  data_frame <- data.frame(X = data[, 1], Y = data[, 2])
  return(data_frame)
}

# Example: Generate data with a correlation of 0.7
as <- generate_correlation(35,.51)

mod1 <- lm(Y~X, data = as)

data <- 
  as |> 
  mutate(
    yhat = predict(mod1) |> as.vector(),
    res = Y - yhat
  )
```

::: panel-tabset
## Scaterrplot

```{r}
data |> 
  ggplot(aes(X, Y)) + 
  geom_point() + 
  theme_minimal() + 
  labs(
    title = "Simple Linear Regression",
    x = "\nX",
    y = "Y\n",
  ) + 
  theme(
    panel.grid = element_blank()
  )
```

## Linear Regression

```{r}
data |> 
  ggplot(aes(X, Y)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() + 
  labs(
    title = "Simple Linear Regression",
    x = "\nX",
    y = "Y\n"
  ) + 
  theme(
    panel.grid = element_blank()
  )
```

## Residual Plot

```{r}
data |> 
  ggplot(aes(X, Y)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) + 
  geom_segment(aes(xend = X, yend = yhat),
               lty = "dashed") + 
  theme_minimal() + 
  labs(
    title = "Simple Linear Regression",
    x = "\nX",
    y = "Y\n"
  ) + 
  theme(
    panel.grid = element_blank()
  )
```
:::

## Regression:

### Equation Component Meanings

-   $Y$ = theoretical prediction of `y`

-   $\hat{Y}$= the actual `y` value for a participant

-   $\beta_0$ = the intercept: the average `y` value

-   $A$ = the A value for a participant

-   $\beta_1$ = a weighted value that is multiplied by `A` and added to $\beta_0$ to estimate `y`.

## Regression Equation: 

### More than one X

$\large{\hat{Y}_i = \beta_0 + \beta_1(A) + \beta_2(B)}...$

## Regression:

### Equation Component Meanings

-   $Y$ = theoretical prediction of `y`

-   $\hat{Y}$= the actual `y` value for a participant

-   $\beta_0$ = the intercept: the average `y` value

-   $A$ = the $A$ value for a participant

-   $\beta_1$ = a weighted value that is multiplied by x and added to $\beta_0$ to estimate y.

-   $B$ = the $B$ value for a participant

-   $\beta_2$ = a weighted value that is multiplied by $B$

## Regression Equation:

## More than one X

$\hat{Y}_i = \beta_0 + \beta_1(A) + \beta_2(B)$

# Interpreting Coefficients

```{r}
test <- 
  tibble(
  money = rnorm(100,55,5),
  x1 = rnorm(100,14,2),
  x2 = rnorm(100,30,2)
) 

mod1 <- lm(money ~ x1+x2, data = test) |> summary() 
  
mod1 |> tbl_regression()
```

## Interpreting coefficients

-   Beta coefficient

-   t-value

-   p-value

## Interpreting coefficients

::: nonincremental
-   Beta coefficient

-   t-value

-   **p-value: is this a significant predictor of y?**
:::

## Interpreting coefficients

::: nonincremental
-   **Beta coefficient: which predictor of y has the more predictive power?**

-   t-value

-   p-value
:::

# $R^2$: Coefficient of Determination

### How much can we account for?

-   $R^2$ is the correlation coefficient (`r`) squared.

-   $R^2$ is always positive

-   In correlation, the amount of `y` accounted for by `x`

## $R^2$: Coefficient of Determination

### How much can we account for?

-   In Regression, $R^2$ refers to the amount of `y` accounted for by all of the `x's`.

-   How much of `y` did we account for?

-   Did we account for a **significant** amount of the variance in `y`?

## $R^2$: Coefficient of Determination

### How much can we account for?

## Interpreting R Squared

::: nonincremental
-   R Squared

-   Adjusted R Squared

-   F-value

-   **p-value: is this a significant predictor of `y`?**
:::

## Interpreting R Squared

::: nonincremental
-   **R Squared: The percentage of `y` accounted for by the predictors**

-   Adjusted R Squared

-   F-value

-   p-value
:::

## Interpreting R Squared

::: nonincremental
-   R Squared

-   **Adjusted R Squared: The percentage of `y` accounted for by the predictors, controlling for using too many predictors**

-   F-value

-   p-value
:::

# Linear Regression Example

## Linear Regression

### Example

> Professor Brocker wants to be able to predict social media use. The Social Media Use Scale measures different motivations and reasons for using Social Media platforms.

## Linear Regression

> Professor Brocker wants to be able to predict social media use.

What are some predictors of social media use?

-   Sense of belonging

-   Age

## Regression Equation

$$\large{Y_i = \beta_0 + \beta_1A + \beta_2B~ + e_i}$$

## Regression Equation

### Theoretical

$$\large{\color{red}{Y_i} = \color{green}{\beta_0} + \color{orange}{\beta_1}\color{blue}{(A)} + \color{purple}{\beta_2(B)}\color{pink}{X}~ + e_i}$$

1.  The predicted degree of social media use for a participant "`i`"

2.  Intercept: Average social media use when all other predictors are 0.

3.  Coefficient for $A$

4.  What is the sense of belonging?

5.  Coefficient for $B$

6.  Age

7.  Randomness

## Regression Equation

### Computational

$\hat{Y}_i = \beta_0 + \beta_1(A) + \beta_2(B)$

## Linear Regression

-   This form of regression is called linear regression.

## Example

### Model Coefficients

```{r}
library(gt)
library(broom)
# Set seed for reproducibility
set.seed(123)

# Generate 30 participants
participants <- paste0("P", 1:30)

# Age: Randomly sampled from 18 to 50
age <- sample(18:50, 30, replace = TRUE)

# Sense of Belonging: Continuous scale from 0 to 10
belonging <- round(runif(30, min = 0, max = 10), 2)

# Generate social media use (hours per day) using a model
# Assume younger people and those with higher belonging use social media more
social_media_use <- 5 - (0.1 * age) + (0.3 * belonging) + rnorm(30, mean = 0, sd = 0.5)

# Create data frame
df_sob <- data.frame(
  Participant = participants,
  Age = age,
  Sense_of_Belonging = belonging,
  Social_Media_Use = round(social_media_use, 2)
)

# Create Model
sm_mod <- 
  lm(Social_Media_Use ~ 
     Age + Sense_of_Belonging, 
   data = df_sob) |> 
  tbl_regression(
    intercept = TRUE
  ) |> 
  as_gt() |> 
  tab_header(title = "Social Media Use Model")

# Show Model
sm_mod

# Show Other Summaries
model_summary <- 
  lm(Social_Media_Use ~ 
     Age + Sense_of_Belonging, 
   data = df_sob) |> 
    summary()

# Create a data frame with the desired values
mod2 <- data.frame(
  Metric = c("Residual Standard Error", "Multiple R-squared", "Adjusted R-squared", "F-statistic", "p"),
  Value = c(
    model_summary$sigma,                        # Residual Standard Error
    model_summary$r.squared,                    # Multiple R-squared
    model_summary$adj.r.squared,                # Adjusted R-squared
    model_summary$fstatistic[1],                # F-statistic
    pf(model_summary$fstatistic[1], model_summary$fstatistic[2], model_summary$fstatistic[3], lower.tail = FALSE)  # p-value
  )
)

# Convert to a gt table
mod2 |> 
  gt() |> 
  fmt_auto() |> 
  tab_header(
    title = md("**Regression Model Summary**"),
    subtitle = md("*Predicting Social Media Use for Age and Sense of Belonging*")
  )
```

# Reporting Regression Findings

## Example

First, report adjusted $R^2$ and it's corresponding p:

-   Our model predicted a significant amount of variance in social media use, *adjusted* $R^2$ = 0.86, p\* \< 0.001.

-   Age, sense of belonging, predicted a significant amount of variance in social media use, *adjusted* $R^2$ = 0.86, p \< 0.001.

## Example

```{r}
sm_mod
```

Then report individual betas and their p's:

-   Sense of belonging (*beta = 0.29, p = 0.001*) significantly predicted social media use.

-   Age significantly predicted likelihood of social media use (beta = -.10, p \<.001)

-   It's common to report betas and p's in text as well as in a table.

# Categorical Variables in Regression

## Categorical variables

```{r}
# Set seed for reproducibility
set.seed(123)

# Generate 30 participants
n <- 30

# Continuous predictors
duration_tv <- round(runif(n, 0.5, 5), 2)  # Hours of TV watched
cups_of_coffee <- sample(0:5, n, replace = TRUE)  # Cups of coffee
prior_sleep <- round(rnorm(n, mean = 7, sd = 1.5), 2)  # Hours of sleep

# Categorical predictor: TV Show Genre (Drama, Comedy, Documentary)
tv_genre <- sample(c("Drama", "Comedy", "Documentary"), n, replace = TRUE)

# Assign numeric values for regression (Dummy coding: Comedy as reference)
tv_genre_num <- ifelse(tv_genre == "Drama", 1, ifelse(tv_genre == "Documentary", 2, 0))

# Generate energy level using a regression model
energy_level <- 3.5 + 
  (1.2 * duration_tv) + 
  (0.8 * cups_of_coffee) + 
  (-0.5 * prior_sleep) + 
  (1.5 * tv_genre_num) +  # Drama increases energy, Documentary increases it more
  rnorm(n, mean = 0, sd = 1)  # Add some random noise

# Round energy levels for readability
energy_level <- round(energy_level, 2)

# Create a data frame
df_cat <- data.frame(
  Participant = paste0("P", 1:n),
  Duration_TV = duration_tv,
  Cups_of_Coffee = cups_of_coffee,
  Prior_Sleep = prior_sleep,
  TV_Genre = tv_genre,
  Energy_Level = energy_level
)
```

## Fit the Model

```{r}
model_3 <- lm(Energy_Level ~ Duration_TV + Cups_of_Coffee + Prior_Sleep + TV_Genre, data = df_cat)

tbl_regression(model_3,intercept = TRUE) |> 
  as_gt() |> 
  tab_header(title = md("**Energy Level Model**"),
             subtitle = md("*What is the best predictor?*"))


```

## Horror Movies

```{r}
library(janitor)

hm <- read.csv("Horror Movie Data https___notawfulandboring.blogspot.com_ - Data.csv")

hm_cln <- 
  hm |> 
  clean_names() |> 
  dplyr::select(-movie,-avg_resting_heart_rate_bpm)

lm(scare_score ~ ., data = hm_cln) |> 
tbl_regression(label = 
                 list("ranking" = "Ranking",
                    "avg_movie_heart_rate_bpm" = "Avg. Heart Rate (BPM)",
                   "overall_difference_bpm" = "Overall Difference (BPM)" ,
                   "hrv_difference" = "HRV Difference" ,
                   "highest_spike_bpm" = "Highest Spike",
                   "this_film_is_a_sequel" = "Sequel",
                   "this_film_has_at_least_one_sequel" = "At Least One Sequel",
                  "rotten_tomato_score" = "Rotten Tomato Score" ,
                   "year" = "Year" )) 
```

## Review Questions

**Multiple-Choice Questions**

**1. What does the slope in a simple linear regression equation represent?**

A\) The value of Y when X = 0

B\) The predicted change in Y for a one-unit increase in X

C\) The strength of the correlation between X and Y

D\) The proportion of variance in Y explained by X

**Answer:** B

**2. If a linear regression model has an R² value of 0.85, what does this mean?**

A\) 85% of the variability in X is explained by Y

B\) The regression model is 85% accurate

C\) 85% of the variability in Y is explained by X
D) The relationship between X and Y is statistically significant

**Answer:** C

**3. A researcher runs a regression model and finds the following equation:**

What does the number **5** represent?

A\) The slope of the regression line

B\) The predicted value of Y when X = 0

C\) The effect size of X on Y

D\) The p-value of the regression

**Answer:** B

**4. Which of the following would indicate that a multiple regression model is overfitting?**

A\) A high R² value on the training data but poor performance on new data

B\) A non-significant p-value for the intercept

C\) The presence of a negative coefficient in the model
D) A low standard error for the slope

**Answer:** A

**5. A regression model predicts salary based on years of experience. The p-value for the slope is 0.0003. What does this mean?**

A\) Years of experience does not significantly predict salary

B\) Years of experience significantly predicts salary at the α = 0.05 level

C\) The slope coefficient is 0.0003

D\) The model is not linear

**Answer:** B

## **Open-Ended Questions (Show Your Work)**

**6. Given the following data, calculate the least squares regression equation:**

|                       |                    |
|-----------------------|--------------------|
| **X (Hours Studied)** | 	**Y (Exam Score)** |
| 2                     | 50                 |
| 4                     | 60                 |
| 6                     | 70                 |
| 8                     | 80                 |
| 10                    | 90                 |

**Show all calculations to determine the slope and intercept.**

**7. A multiple regression model is given by:**

$\hat{Y} = 3 + 1.5X_1 - 0.7X_2$

where:

	• \$X_1\$ = Number of hours spent studying

	• \$X_2\$ = Number of hours spent watching TV

**a)** Predict the outcome if a student studies for 6 hours and watches TV for 2 hours.

3 + (1.5*6) + (.7*2)

13.4

**b)** Interpret the coefficient of .

**8. A researcher runs a simple linear regression and obtains the following output:**

**Predictor**	**Coefficient**	**Standard Error**	**p-value**

Intercept	4.2	0.5	0.001

X	1.8	0.3	0.0005

**a)** Write the regression equation.

**b)** Interpret the slope in context.

**c)** If , predict .

**9. A study investigates the relationship between sleep (X) and energy levels (Y). The model output is:**

	•	What is the residual for a participant who slept **5 hours** and had an energy level of **18**?

**10. You are given a dataset with 100 observations and decide to build a regression model predicting house prices based on square footage, number of bedrooms, and crime rate.**

	•	List two possible **assumptions of linear regression** that should be checked before interpreting the model.

	•	Explain how violating each assumption could affect the model’s accuracy.
