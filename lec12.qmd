---
title: "Regression Part II"
subtitle: "Lecture 12"
author: "Dave Brocker"
footer: "⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡"
institute: "Farmingdale State College"
format: 
  revealjs:
    theme: custom.scss
    scrollable: true
    incremental: true   
    touch: true
    chalkboard: true
    lightbox: true
    code-fold: true
    drop:
      engine: webr
      webr:
        packages:
         - ggplot2
         - dplyr
revealjs-plugins:
  - drop
---

## Types of Analysis

-   Univariate: One variable, like the mean

-   Bivariate: Two variables, like correlation

-   Multivariate: More than 2 variables

## Regression

-   Regression is the multivariate version of correlation.

-   Correlation is the bivariate version of regression.

-   We're doing the same thing in regression as we do in correlation, BUT there are more than 2 variables.

## Regression terminology

In regression we will choose one variable to predict.

-   We call this `y`.

-   `Y` is the outcome variable.

-   `Y` is the predicted variable.

## Regression terminology

In regression we will choose two or more variables we with believe predict y.

-   We call these $x_1$, $x_2$, etc.

-   Some people call these `x` and `z`.

-   `X` is the predictor variable.

## Regression terminology

When we predict Y with `X`, we say;

. . .

::: {style="font-size:90px;"}
We regress `y` on the `x`.
:::

## Regression Equation

### Step-by-Step

$$
\large{\color{red}{Y_i} = \color{green}{b_0} + \color{blue}{b_1}X_{1i}+\color{orange}{e_i}}
$$

::: fragment
[1. The predicted value of y for a particular participant (i)]{style="color:red"}
:::

::: fragment
[2. Intercept]{style="color:green"}
:::

::: fragment
[3. Weight]{style="color:blue"}
:::

::: fragment
[4. The Value of a particular participant (i) on the measure of $X_1$]{style="color:black"}
:::

::: fragment
[5. Error. Random, and therefore cannot be predicted]{style="color:orange"}
:::

## Regression Equation

$$
\large{\color{red}{Y_i} = \color{green}{b_0} + \color{blue}{b_1}X_{1i}}
$$

::: fragment
$$\hat{Y} = b_0 + b_1X_{1i}$$
:::

::: fragment
$$Y = ax + b$$
:::

## Linear Regression

### Line of Best Fit

```{r}
library(dplyr)
library(ggplot2)
library(MASS)

generate_correlation <- function(n, rho) {
  # Define the mean and covariance matrix
  mean_vector <- c(0, 0)
  covariance_matrix <- matrix(c(1, rho, rho, 1), nrow = 2)
  
  # Generate correlated data using mvrnorm from the MASS package
  data <- mvrnorm(n = n, mu = mean_vector, Sigma = covariance_matrix)
  
  # Convert to a data frame
  data_frame <- data.frame(X = data[, 1], Y = data[, 2])
  return(data_frame)
}

# Example: Generate data with a correlation of 0.7
n <- 100  # Number of data points
rho <- 0.82  # Desired correlation
data <- generate_correlation(n, rho)

data <- data*23

lm(Y~X, data = data) |> summary()

data |> 
  ggplot(aes(X, Y)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) + 
  theme_minimal() + 
  labs(
    title = "Simple Linear Regression",
    x = "\nX",
    y = "Y\n",
    caption = "Y = .72 + 831x"
  ) + 
  theme(
    panel.grid = element_blank()
  )

```

## Regression:

### Equation Component Meanings

-   $Y$ = theoretical prediction of `y`

-   $\hat{Y}$= the actual `y` value for a participant

-   $b_0$ = the intercept: the average `y` value

-   $X_1$ = the `x` value for a participant

-   $b_1$ = a weighted value that is multiplied by `x` and added to $b_0$ to estimate `y`.

## Regression Equation: More than one X

$$\large{\hat{Y}_i = b_0 + b_1X_1i + b_2X_2i}$$

## Regression:

### Equation Component Meanings

-   $Y$ = theoretical prediction of `y`

-   $\hat{Y}$= the actual `y` value for a participant

-   $b_0$ = the intercept: the average `y` value

-   $X_1$ = the $X_1$ value for a participant

-   $b_1$ = a weighted value that is multiplied by x and added to $b_0$ to estimate y.

-   $X_2$ = the $x_2$ value for a participant

-   $b_2$ = a weighted value that is multiplied by $x_2$

## Regression Equation:

## More than one X

$\hat{Y}_i = b_0 + b_1X_1i + b_2X_2i$

# Interpreting Coefficients

## Interpreting coefficients

```{r}
#| 

library(dplyr)
library(huxtable)

test <- 
  tibble(
  money = rnorm(100,55,5),
  x1 = rnorm(100,14,2),
  x2 = rnorm(100,30,2)
) 

mod1 <- lm(money ~ x1+x2, data = test) |> summary() 
  
tab1 <- 
  huxreg(
  "Money"=mod1,
  error_pos = "right",
  statistics = 
    c('# Observations' = 'nobs', 
      'R squared' = 'r.squared', 
      'Adjusted R squared' = 'adj.r.squared',
      'F statistic' = 'statistic',
      'P value' = 'p.value'),
  bold_signif = .05,
  note = "{stars}"
  ) |> 
  theme_striped() |> 
  set_all_padding(0) |> 
  set_width(.8)
```

## Interpreting coefficients

-   Unstandardized `b` coefficient

-   Beta coefficient

-   t-value

-   p-value

## Interpreting coefficients

::: nonincremental
-   Unstandardized `b` coefficient

-   Beta coefficient

-   t-value

-   **p-value \<--- is this a significant predictor of y?**
:::

## Interpreting coefficients

::: nonincremental
-   Unstandardized `b` coefficient

-   **Beta coefficient \<--- which predictor of y has the more predictive power?**

-   t-value

-   p-value
:::

## Interpreting coefficients

### Intercept

```{r}
tab1 |> 
  set_all_border_colors(row = 2, value = "red") |>
  set_all_border_styles(row = 2, value = "solid")
```

## Interpreting coefficients

### First Predictor Variable

```{r}
tab1 |> 
  set_all_border_colors(row = 3, value = "red") |>
  set_all_border_styles(row = 3, value = "solid")
```

## Interpreting coefficients

### Second Predictor Variable

```{r}
tab1 |> 
  set_all_border_colors(row = 4, value = "red") |>
  set_all_border_styles(row = 4, value = "solid")
```

## Interpreting coefficients

### Sample Size

```{r}
tab1 |> 
  set_all_border_colors(row = 5, value = "red") |>
  set_all_border_styles(row = 5, value = "solid")
```

## Interpreting coefficients

### R Squared

```{r}
tab1 |> 
  set_all_border_colors(row = 6, value = "red") |>
  set_all_border_styles(row = 6, value = "solid")
```

## Interpreting coefficients

### Adjusted R Squared

```{r}
tab1 |> 
  set_all_border_colors(row = 7, value = "red") |>
  set_all_border_styles(row = 7, value = "solid")
```

## Interpreting coefficients

### F Statistic

```{r}
tab1 |> 
  set_all_border_colors(row = 8, value = "red") |>
  set_all_border_styles(row = 8, value = "solid")
```

## Interpreting coefficients

### P Value

```{r}
tab1 |> 
  set_all_border_colors(row = 9, value = "red") |>
  set_all_border_styles(row = 9, value = "solid")
```

# $R^2$

## $R^2$

-   $R^2$ is the correlation coefficient (`r`) squared.

-   $R^2$ is always positive

-   In correlation, the amount of `y` accounted for by `x`

## $R^2$

-   In Regression, $R^2$ refers to the amount of `y` accounted for by all of the `x's`.

-   How much of `y` did we account for?

-   Did we account for a **significant** amount of the variance in `y`?

## $R^2$

![](images/Screenshot%202024-10-21%20at%208.17.56%20AM.png)

## Interpreting R Squared

-   R Squared

-   Adjusted R Squared

-   F-value

-   p-value

## Interpreting R Squared

::: nonincremental
-   R Squared

-   Adjusted R Squared

-   F-value

-   **p-value \<--- is this a significant predictor of `y`?**
:::

## Interpreting R Squared

::: nonincremental
-   **R Squared \<--- The percentage of `y` accounted for by the predictors**

-   Adjusted R Squared

-   F-value

-   p-value
:::

## Interpreting R Squared

::: nonincremental
-   R Squared

-   **Adjusted R Squared \<--- The percentage of `y` accounted for by the predictors, controlling for using too many predictors**

-   F-value

-   p-value
:::

# Linear Regression Example

## Linear Regression

### Example

> Professor Brocker wants to be able to predict social media use. The Social Media Use Scale measures different motivations and reasons for using Social Media platforms.

## Linear Regression

> Professor Brocker wants to be able to predict social media use.

What are some predictors of social media use?

-   Sense of belonging

-   Age

## Regression Equation

$$\large{Y_i = b_0 + b_1X_1i + b_2X_2i~ + e_i}$$

## Regression Equation

### Theoretical

$$\large{\color{red}{Y_i} = \color{green}{b_0} + \color{orange}{b_1}\color{blue}{X_{1i}} + \color{purple}{b_2}\color{pink}{X_{2i}}~ + e_i}$$

1.  The predicted degree of social media use for a participant "`i`"

2.  Intercept: Average social media use

3.  Coefficient for $X_1$

4.  What is the sense of belonging?

5.  Coefficient for $X_2$

6.  Age

7.  Randomness

## Regression Equation

### Computational

$\hat{Y}_i = b_0 + b_1X_1i + b_2X_2i$

## Linear Regression

-   This form of regression is called linear regression.

## Example

### Model Coefficients

![](images/model.png)

## Example

### Model Summary

![](images/modelsum.png)

# Reporting Regression Findings

## Example

First, report adjusted $R^2$ and it's corresponding p:

-   Our model predicted a significant amount of variance in social media use, *adjusted* $R^2$ = 0.29, p\* \< 0.001.

-   Age, sense of belonging, predicted a significant amount of variance in social media use, *adjusted* $R^2$ = 0.29, p\* \< 0.001.

## Example

![](images/model.png)

Then report individual betas and their p's:

-   Sense of belonging (*beta = 0.912, p = 0.001*) significantly predicted social media use.

-   Age did not predict likelihood of social media use.

    -   (beta = .112, p = .06)

-   It's common to report betas and p's in text as well as in a table.

# Categorical Variables in Regression

## Categorical variables

### Discrete, not continuous:

-   Groups

-   Categories

-   Labels

## Categorical variables

-   If your *only* predictor is a categorical variable, you should run a t-test or ANOVA.

-   Sometimes researchers want to examine how several variables predict one outcome.

-   We did this with high thoughts:

    -   Age

    -   Sense of Belonging

    -   Social Media Use

## Categorical variables in Regression

Sometimes researchers want to examine how several variables predict one outcome.

Sometimes one of these variables is categorical.

> Example: How many episodes of a show can Professor Brocker watch before he passes out on his couch?

## Categorical variables in Regression

> Dependent variable: How many episodes of a show can Professor Brocker watch before he passes out on his couch?

::::: columns
::: {.column width="50%"}
Predictors:

-   Time of night

-   Cups of coffee consumed

-   Is it about time travel?
:::

::: {.column width="50%"}
Variable Type:

-   Continuous

-   Continuous

-   *Categorical*
:::
:::::

## Categorical variables in Regression

Dependent variable: How many episodes of a show can Professor Brocker watch before he passes out on his couch?

Predictors:

-   Time of night: As it gets later, the number of episodes will decrease (negative correlation)

-   Cups of coffee consumed: As cups of coffee increases, number of episodes will increase (positive correlation)

-   Is the show about time travel? If so, number of episodes will increase.

## Dummy coding

-   Assign each category a number value.

    -   0 = No

    -   1 = Yes

## Dummy coding

-   Assign each category a number value.

    -   0 = No, The show is not about time travel.

    -   1 = Yes, The show is about time travel.

## Interpreting dummy coded variables

![](images/dummyvar.png)

## Report the findings

-   Time significantly predicted the outcome variable in that as the time got later in the evening, number of episodes decreased (*beta = -0.412, p = 0.04*).

## Interpreting dummy coded variables

![](images/dummyvar.png)

## Report the findings

-   Time significantly predicted the outcome variable in that as the time got later in the evening, number of episodes decreased (*beta = -0.412, p = 0.04*).

-   Coffee was not a significant predictor of number of episodes.

## Interpreting dummy coded variables

![](images/dummyvar.png)

## Report the findings

-   Time significantly predicted the outcome variable in that as the tim got later in the evening, number of episodes decreased (beta = -0.412, p = 0.04).

-   Coffee was not a significant predictor of number of episodes.

-   Show content significantly predicted watching more episodes (beta = 0.912, p \< 0.001).

## Dummy coding

-   Gender:

    -   0 = Male

    -   1 = Female

## Interpreting dummy coded variables

![](images/dummyvar.png)

## Interpreting dummy coded gender

-   Beta is positive (beta = 0.912, p \< 0.001).

-   Whatever is coded as 1 is higher on the the dependent variable.

-   Women reported significantly more enjoyment of True Crime than men.

## Categorical variables in regression

-   Y = How many days would you survive in the zombie apocalypse?

-   Y = How many days would you survive in the zombie apocalypse?

-   X1 = Crossbow skills on a scale of 1 (what's a crossbow?) to 10 (bullseye)

## Categorical variables in regression

-   Y = How many days would you survive in the zombie apocalypse?

-   X1 = Crossbow skills on a scale of 1 (what's a crossbow?) to 10 (bullseye)

-   X2 = \# of episodes of the Walking Dead watched

-   X3 = Do you have a weapon? Y/N

## Categorical variables in regression

-   Crossbow skills: As crossbow skills go up, days survived will go up.

-   WD episodes: As episodes goes up, days survived will go up.

-   Weapon:

    -   0 = No weapon :(

    -   1 = Weapon! :)

## Categorical variables in regression

-   Crossbow skills: As crossbow skills go up, days survived will go up.

-   WD episodes: As episodes goes up, days survived will go up.

-   Weapon: Individuals who have weapon will survive more days.

-   How will we know if this is true?

    -   Beta will be positive if having a weapon increases days.

    -   Beta will be negative if having a weapon decreases days.

## Horror Movies

```{r}
library(janitor)

hm <- read.csv("Horror Movie Data https___notawfulandboring.blogspot.com_ - Data.csv")

hm_cln <- 
  hm |> 
  clean_names() |> 
  dplyr::select(-movie,-avg_resting_heart_rate_bpm)

lm(scare_score ~ ., data = hm_cln) |> 
  summary() |> 
  huxreg(bold_signif = .05,
         note = "{stars}",
         error_pos = "same",
         statistics = c("nobs","r.squared"),
         coefs = c("Ranking" = "ranking",
                   "Avg. Heart Rate (BPM)" = "avg_movie_heart_rate_bpm",
                   "Overall Difference (BPM)" = "overall_difference_bpm",
                   "HRV Difference" = "hrv_difference",
                   "Highest Spike" ="highest_spike_bpm",
                   "Sequel" ="this_film_is_a_sequelyes",
                   "At Least One Sequel" ="this_film_has_at_least_one_sequelyes",
                  "Rotten Tomato Score" ="rotten_tomato_score",
                   "Year" ="year")) |> 
  theme_striped() |> 
  print_screen()


```
